# Ejemplo de Implementaci칩n de un Servicio de creaci칩n de cat치lgo de moda para b칰squeda.

### MLPOS2 - CEIA - FIUBA

Supongamos que trabajamos para **ML Models and something more Inc.**, la cual ofrece un servicio de creaci칩n de
cat치logo de productos de moda y de b칰squeda avanzada sobre el mismo tanto por texto como por im치genes utilizando el
modelo CLIP-ViT. Internamente, tanto para realizar tareas de DataOps como de MLOps, la empresa cuenta con varios
servicios que ayudan a ejecutar las acciones necesarias. Tambi칠n dispone de un Data Lake en S3, para este caso,
simularemos un S3 utilizando MinIO.

Para simular esta empresa, utilizaremos Docker y, a trav칠s de Docker Compose, desplegaremos varios contenedores que
representan distintos servicios en un entorno productivo.

La implementaci칩n de ese servicio incluye:

- [Apache Airflow](https://airflow.apache.org/)
    - Un DAG que obtiene datos de un repositorio p칰blico o de un repositorio local de productos de moda, realiza
      limpieza y feature engineering y guarda en un bucket s3://data los datos separados para entrenamiento y pruebas.
      MLflow hace seguimiento de este procesamiento.
    - Un DAG que realiza experimentos de fine-tuning del modelo CLIP con el dataset y se calculan m칠tricas obtenidas. Se
      compara el nuevo modelo ajustado con el mejor modelo hasta ahora, y si es mejor, se reemplaza. Todo se lleva a
      cabo siendo registrado en MLflow.
    - Un DAG que recrea la base de datos de producci칩n, descargando datos de un repositorio p칰blico, guardando los
      metadatos en una base de datos PostgreSQL (tabla fashion_files) y luego actualizando los embeddings para cada
    - producto utilizando el mejor modelo registrado en MLflow.
- [MLflow](https://mlflow.org/) para llevar registro de los experimentos, datasets y modelos entrenados. En especial 
  para registrar los mejores modelos de finetuned CLIP a ser utilizados en producci칩n y sus m칠tricas.
- GraphQL para realizar consultas de los productos disponibles en PostgreSQL y ejecutar b칰squedas por texto mediante
  llamada gRPC al servicio de modelo.
- Rest api para realizar consultas de los productos disponibles en PostgreSQL y ejecutar b칰squedas por texto o im치genes
  mediante llamada gRPC al servicio de modelo.
- Servicio de modelo que sirve el mejor modelo registrado en MLflow para realizar b칰squedas por texto o im치genes
  mediante llamadas gRPC.
- [MinIO](https://min.io/) para almacenar los buckets.
- Base de datos relacional [PostgreSQL](https://www.postgresql.org/) para almacenar los productos.
- Base de dato key-value [ValKey](https://valkey.io/)
- Aprendizaje federado y seguridad. (TBD seg칰n pr칩xima clase)
- Orquestaci칩n del servicio en contenedores utilizando Docker.

![Diagrama de servicios](mlops2_architecture.png)

Por defecto, cuando se inician los multi-contenedores, se crean los siguientes buckets:

- `s3://data` (usado por Airflow para guardar los datos de entrenamiento y pruebas).
- `s3://mlflow` (usada por MLflow para guardar los artefactos).
- `s3://prod` (usada para almacenar las im치genes de la base de datos de producci칩n).
- `s3://tmp` (usada para almacenar temporalmente las im치genes subidas por usuarios al realizar b칰squedas de productos
  por im치gen).

y las siguientes bases de datos:

- `mlflow_db` (usada por MLflow).
- `airflow` (usada por Airflow).
- `fashion` (usada para almacenar los metadatos de los productos de moda).

## Instalaci칩n

1. Para poder levantar todos los servicios, primero instala [Docker](https://docs.docker.com/engine/install/) en tu
   computadora (o en el servidor que desees usar).
2. Clona este repositorio.
3. Si est치s en Linux o MacOS, en el archivo `.env`, reemplaza `AIRFLOW_UID` por el de tu usuario o alguno que consideres
   oportuno (para encontrar el UID, usa el comando `id -u <username>`). De lo contrario, Airflow dejar치 sus carpetas
   internas como root y no podr치s subir DAGs (en `airflow/dags`) o plugins, etc.
4. En la carpeta ra칤z de este repositorio, ejecuta:

```bash
docker compose --profile all up
```

5. Una vez que todos los servicios est칠n funcionando (verifica con el comando `docker ps -a` que todos los servicios
   est칠n healthy o revisa en Docker Desktop), podr치s acceder a los diferentes servicios mediante:
    - Apache Airflow: http://localhost:18080
    - MLflow: http://localhost:5001
    - MinIO: http://localhost:9001 (ventana de administraci칩n de Buckets)
    - Rest API: http://localhost:8800/
    - GraphQL API: http://localhost:8801/.
    - GraphQL API playground: http://localhost:8801/graphql

Si est치s usando un servidor externo a tu computadora de trabajo, reemplaza `localhost` por su IP (puede ser una privada
si tu servidor est치 en tu LAN o una IP p칰blica si no; revisa firewalls u otras reglas que eviten las conexiones).

## Apagar los servicios

Estos servicios ocupan cierta cantidad de memoria RAM y procesamiento, por lo que cuando no se est치n utilizando, se
recomienda detenerlos. Para hacerlo, ejecuta el siguiente comando:

```bash
docker compose --profile all down
```

Si deseas no solo detenerlos, sino tambi칠n eliminar toda la infraestructura (liberando espacio en disco), utiliza el
siguiente comando:

```bash
docker compose down --rmi all --volumes
```

Nota: Si haces esto, perder치s todo en los buckets y bases de datos.

## Aspectos espec칤ficos de Airflow

### Variables de entorno

Airflow ofrece una amplia gama de opciones de configuraci칩n. En el archivo `docker-compose.yaml`, dentro de
`x-airflow-common`, se encuentran variables de entorno que pueden modificarse para ajustar la configuraci칩n de Airflow.
Pueden a침adirse [otras variables](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html).

### Uso de ejecutores externos

Actualmente, para este caso, Airflow utiliza un
ejecutor [celery](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/celery.html), lo que
significa que las tareas se ejecutan en otro contenedor.

### Uso de la CLI de Airflow

Si necesitan depurar Apache Airflow, pueden utilizar la CLI de Apache Airflow de la siguiente manera:

```bash
docker compose --profile all --profile debug up
```

Una vez que el contenedor est칠 en funcionamiento, pueden utilizar la CLI de Airflow de la siguiente manera,
por ejemplo, para ver la configuraci칩n:

```bash
docker-compose run airflow-cli config list      
```

Para obtener m치s informaci칩n sobre el comando, pueden
consultar [aqui](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html).

### Variables y Conexiones

Si desean agregar variables para accederlas en los DAGs, pueden hacerlo en `secrets/variables.yaml`. Para obtener
m치s [informaci칩n](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/variables.html),
consulten la documentaci칩n.

Si desean agregar conexiones en Airflow, pueden hacerlo en `secrets/connections.yaml`. Tambi칠n es posible agregarlas
mediante la interfaz de usuario (UI), pero estas no persistir치n si se borra todo. Por otro lado, cualquier conexi칩n
guardada en `secrets/connections.yaml` no aparecer치 en la UI, aunque eso no significa que no exista. Consulten la
documentaci칩n para obtener m치s
[informaci칩n](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/connections.html).

## Conexi칩n con los buckets

Dado que no estamos utilizando Amazon S3, sino una implementaci칩n local de los mismos mediante MinIO, es necesario
modificar las variables de entorno para conectar con el servicio de MinIO. Las variables de entorno son las siguientes:

```bash
AWS_ACCESS_KEY_ID=minio   
AWS_SECRET_ACCESS_KEY=minio123 
AWS_ENDPOINT_URL_S3=http://localhost:90000
```

MLflow tambi칠n tiene una variable de entorno que afecta su conexi칩n a los buckets:

```bash
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
```

Aseg칰rate de establecer estas variables de entorno antes de ejecutar tu notebook o scripts en tu m치quina o en cualquier
otro lugar. Si est치s utilizando un servidor externo a tu computadora de trabajo, reemplaza localhost por su direcci칩n
IP.

Al hacer esto, podr치s utilizar `boto3`, `awswrangler`, etc., en Python con estos buckets, o `awscli` en la consola.

Si tienes acceso a AWS S3, ten mucho cuidado de no reemplazar tus credenciales de AWS. Si usas las variables de entorno,
no tendr치s problemas.

## Valkey

La base de datos Valkey es usada por Apache Airflow para su funcionamiento. Tal como est치 configurado ahora no esta
expuesto el puerto para poder ser usado externamente. Se puede modificar el archivo `docker-compose.yaml` para
habilitaro.

## Actualizaciones

Para utilizar este repositorio y cargar datos del dataset `ashraq/fashion-product-images-small`, se puede ejecutar el
siguiente comando:

```bash
poetry run python src/data/dataset_loader.py
```

Este script realiza las siguientes tareas:

- Descarga un conjunto de im치genes del dataset desde Hugging Face.
- Guarda las im치genes en un bucket S3 utilizando MinIO como almacenamiento.
- Genera un 칤ndice en PostgreSQL, preservando las columnas originales del dataset y agregando campos adicionales de
  metadatos.

---

### Verificar los datos indexados en PostgreSQL

Una vez ejecutado el script, se puede consultar el 칤ndice generado en PostgreSQL con:

```bash
psql -h localhost -p 15432 -U airflow -d airflow
```

Y luego ejecutar la siguiente consulta SQL:

```sql
SELECT * FROM fashion_files LIMIT 5;
```

---

### Instalaci칩n de `psql` en macOS

Para poder ejecutar `psql`, es necesario tenerlo instalado. En macOS, se puede instalar con:

```bash
brew install libpq
```

Como `libpq` es un paquete *keg-only*, no se agrega autom치ticamente al `PATH`. Para solucionarlo, se debe ejecutar:

```bash
echo 'export PATH="/opt/homebrew/opt/libpq/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc
```

Verific치 que haya quedado correctamente configurado con:

```bash
which psql
psql --version
```

La salida esperada deber칤a ser similar a:

```
/opt/homebrew/opt/libpq/bin/psql
psql (PostgreSQL) 17.5
```

Con esto ya pod칠s volver a ejecutar el comando `psql` y realizar consultas sobre la tabla `fashion_files`.

## API GraphQL

Este proyecto expone una API GraphQL desarrollada con **Strawberry** y **FastAPI**, que permite consultar los metadatos
de los archivos indexados del dataset `ashraq/fashion-product-images-small`.

### 游댋 Consultar la API

Docker levantar치 la API en el puerto 8801:8801. Accediendo al endpoint `/graphql` se podr치n ejecutar consultas usando la
UI.

---

### 游늶 Queries disponibles

#### 游댳 `allFiles`

Devuelve todos los registros indexados (limitado por defecto en el backend).

```graphql
{
  allFiles {
    id
    filename
    gender
    masterCategory
    baseColour
  }
}
```

---

#### 游댳 `filesByFilters(...)`

Consulta flexible con m칰ltiples filtros opcionales y paginaci칩n:

**Par치metros disponibles:**

- `masterCategory` (String)
- `gender` (String)
- `baseColour` (String)
- `season` (String)
- `year` (String)
- `limit` (Int, por defecto: 50)
- `offset` (Int, por defecto: 0)

**Ejemplos:**

```graphql
{
  filesByFilters(gender: "Women", season: "Winter", limit: 10) {
    id
    filename
    productDisplayName
  }
}
```

---

#### 游댳 `search(...)`

Realiza una b칰squeda avanzada por texto de productos de modda, retornando los productos que m치s coincidan con la
descripci칩n ingresada.

**Par치metros disponibles:**

- `description` (String)
-

**Ejemplos:**

```graphql
mutation{
  search(description:"soccer jersey"){
    id
    filename
    productDisplayName
  }
}
```

---

### 游닍 Campos disponibles en cada archivo (`FashionFile`)

- `id`
- `filename`
- `s3Path`
- `masterCategory`
- `subCategory`
- `articleType`
- `baseColour`
- `season`
- `year`
- `usage`
- `gender`
- `productDisplayName`
- `dataset`
- `created_at`

---

> 游눠 Nota: los archivos f칤sicos est치n almacenados en un bucket S3 (MinIO), y los campos representan metadatos extra칤dos
> al momento de la carga del dataset.

## API REST

Este proyecto expone una API REST desarrollada con **FastAPI**, que permite realizar b칰squedas de productos de moda
tanto por texto como por im치genes.

### 游댋 Consultar la API

Docker levantar치 la API en el puerto 8800. Debe utilizarse comando curl o postman para realizar las consultas.

---

### 游늶 Queries disponibles

#### 游댳 `POST /search/description`

Realiza una b칰squeda avanzada por texto de productos de moda, retornando los productos que m치s coincidan con la
descripci칩n ingresada.

```commandline
curl --location 'http://localhost:8800/search/description' \
    --header 'Content-Type: application/json' \
    --data '{"description": "white shoes"}'
```

---

#### 游댳 `POST /search/image`

Realiza una b칰squeda avanzada por imagen de productos de moda, retornando los productos que m치s coincidan con la im치gen
ingresada. Para este endpoint, la im치gen es temporalmente subida a s3 previo a realizar la b칰squeda.

**Ejemplos:**

```commandline
curl --location 'http://localhost:8800/search/image' \
    --header 'Content-Type: image/png' \
    --data-binary '/home/user/img.png'
```

---

### 游닍 Campos disponibles

Los campos disponibles en la API REST coinciden con los de la API GraphQL.

## Interfaz de Usuario con Streamlit

Este proyecto incluye una interfaz de usuario interactiva desarrollada con Streamlit que permite realizar b칰squedas de productos de moda tanto por texto como por im치genes de manera sencilla.

### 游댌 Caracter칤sticas

- B칰squeda por descripci칩n de texto
- B칰squeda por im치genes
- Soporte para API REST y GraphQL
- Visualizaci칩n de resultados con im치genes y detalles del producto
- Interfaz intuitiva y f치cil de usar

### 游 C칩mo ejecutar la aplicaci칩n Streamlit

1. Aseg칰rate de que los servicios de backend est칠n en ejecuci칩n (ver secci칩n de Instalaci칩n).

2. Navega hasta el directorio ra칤z del proyecto:
   ```bash
   cd ruta/al/proyecto/amq2-service-ml
   ```

3. Instala las dependencias necesarias (si a칰n no lo has hecho):
   ```bash
   poetry install
   ```

4. Ejecuta la aplicaci칩n Streamlit:
   ```bash
   poetry run streamlit run src/ui/app.py
   ```

5. La aplicaci칩n se abrir치 autom치ticamente en tu navegador predeterminado en `http://localhost:8501`.

### 丘뙖잺 Configuraci칩n

La aplicaci칩n Streamlit se conecta por defecto a los siguientes servicios:
- API REST: `http://localhost:8800`
- API GraphQL: `http://localhost:8801/graphql`

Puedes modificar estas URLs desde la interfaz de usuario en la barra lateral si es necesario.

### 游님 Uso

1. **Selecciona el tipo de API** (REST o GraphQL) en la barra lateral.
2. **Elige el modo de b칰squeda**:
   - **Texto**: Ingresa una descripci칩n del producto que est치s buscando.
   - **Imagen**: Sube una imagen para encontrar productos visualmente similares.
3. Haz clic en "Search" y explora los resultados.
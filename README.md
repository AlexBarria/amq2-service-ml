# Ejemplo de Implementaci칩n de un Servicio de creaci칩n de cat치lgo de moda para b칰squeda.

### MLPOS2 - CEIA - FIUBA

Supongamos que trabajamos para **ML Models and something more Inc.**, la cual ofrece un servicio de creaci칩n de
cat치logo de productos de moda y de b칰squeda avanzada sobre el mismo tanto por texto como por im치genes utilizando el
modelo CLIP-ViT. Internamente, tanto para realizar tareas de DataOps como de MLOps, la empresa cuenta con varios
servicios que ayudan a ejecutar las acciones necesarias. Tambi칠n dispone de un Data Lake en S3, para este caso,
simularemos un S3 utilizando MinIO.

Para simular esta empresa, utilizaremos Docker y, a trav칠s de Docker Compose, desplegaremos varios contenedores que
representan distintos servicios en un entorno productivo.

La implementaci칩n de ese servicio incluye:

- [Apache Airflow](https://airflow.apache.org/)
    - Un DAG que obtiene datos de un repositorio p칰blico o de un repositorio local de productos de moda, realiza
      limpieza y feature engineering y guarda en un bucket s3://data los datos separados para entrenamiento y pruebas.
      MLflow hace seguimiento de este procesamiento.
    - Un DAG que realiza experimentos de fine-tuning del modelo CLIP con el dataset y se calculan m칠tricas obtenidas. Se
      compara el nuevo modelo ajustado con el mejor modelo hasta ahora, y si es mejor, se reemplaza. Todo se lleva a
      cabo siendo registrado en MLflow.
- [MLflow](https://mlflow.org/)
- GraphQL para realizar consultas de los productos disponibles y b칰squedas por texto o im치gen.
- [MinIO](https://min.io/) para almacenar los buckets.
- Base de datos relacional [PostgreSQL](https://www.postgresql.org/) para almacenar los productos.
- Base de dato key-value [ValKey](https://valkey.io/)
- Aprendizaje federado y seguridad. (TBD seg칰n pr칩xima clase)
- Orquestaci칩n del servicio en contenedores utilizando Docker.

![Diagrama de servicios](final_assign.png)

Por defecto, cuando se inician los multi-contenedores, se crean los siguientes buckets:

- `s3://data`
- `s3://mlflow` (usada por MLflow para guardar los artefactos).

y las siguientes bases de datos:

- `mlflow_db` (usada por MLflow).
- `airflow` (usada por Airflow).

## Instalaci칩n

1. Para poder levantar todos los servicios, primero instala [Docker](https://docs.docker.com/engine/install/) en tu
   computadora (o en el servidor que desees usar).
2. Clona este repositorio.
3. Crea las carpetas `airflow/config`, `airflow/dags`, `airflow/logs`, `airflow/plugins`, `airflow/logs`.
4. Si est치s en Linux o MacOS, en el archivo `.env`, reemplaza `AIRFLOW_UID` por el de tu usuario o alguno que consideres
   oportuno (para encontrar el UID, usa el comando `id -u <username>`). De lo contrario, Airflow dejar치 sus carpetas
   internas como root y no podr치s subir DAGs (en `airflow/dags`) o plugins, etc.
5. En la carpeta ra칤z de este repositorio, ejecuta:

```bash
docker compose --profile all up
```

6. Una vez que todos los servicios est칠n funcionando (verifica con el comando `docker ps -a` que todos los servicios
   est칠n healthy o revisa en Docker Desktop), podr치s acceder a los diferentes servicios mediante:
    - Apache Airflow: http://localhost:8080
    - MLflow: http://localhost:5001
    - MinIO: http://localhost:9001 (ventana de administraci칩n de Buckets)
    - API: http://localhost:8800/
    - Documentaci칩n de la API: http://localhost:8800/docs

Si est치s usando un servidor externo a tu computadora de trabajo, reemplaza `localhost` por su IP (puede ser una privada
si tu servidor est치 en tu LAN o una IP p칰blica si no; revisa firewalls u otras reglas que eviten las conexiones).

Todos los puertos u otras configuraciones se pueden modificar en el archivo `.env`. Se invita a jugar y romper para
aprender; siempre puedes volver a clonar este repositorio.

## Apagar los servicios

Estos servicios ocupan cierta cantidad de memoria RAM y procesamiento, por lo que cuando no se est치n utilizando, se
recomienda detenerlos. Para hacerlo, ejecuta el siguiente comando:

```bash
docker compose --profile all down
```

Si deseas no solo detenerlos, sino tambi칠n eliminar toda la infraestructura (liberando espacio en disco), utiliza el
siguiente comando:

```bash
docker compose down --rmi all --volumes
```

Nota: Si haces esto, perder치s todo en los buckets y bases de datos.

## Aspectos espec칤ficos de Airflow

### Variables de entorno

Airflow ofrece una amplia gama de opciones de configuraci칩n. En el archivo `docker-compose.yaml`, dentro de
`x-airflow-common`, se encuentran variables de entorno que pueden modificarse para ajustar la configuraci칩n de Airflow.
Pueden a침adirse [otras variables](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html).

### Uso de ejecutores externos

Actualmente, para este caso, Airflow utiliza un
ejecutor [celery](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/celery.html), lo que
significa que las tareas se ejecutan en otro contenedor.

### Uso de la CLI de Airflow

Si necesitan depurar Apache Airflow, pueden utilizar la CLI de Apache Airflow de la siguiente manera:

```bash
docker compose --profile all --profile debug up
```

Una vez que el contenedor est칠 en funcionamiento, pueden utilizar la CLI de Airflow de la siguiente manera,
por ejemplo, para ver la configuraci칩n:

```bash
docker-compose run airflow-cli config list      
```

Para obtener m치s informaci칩n sobre el comando, pueden
consultar [aqui](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html).

### Variables y Conexiones

Si desean agregar variables para accederlas en los DAGs, pueden hacerlo en `secrets/variables.yaml`. Para obtener
m치s [informaci칩n](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/variables.html),
consulten la documentaci칩n.

Si desean agregar conexiones en Airflow, pueden hacerlo en `secrets/connections.yaml`. Tambi칠n es posible agregarlas
mediante la interfaz de usuario (UI), pero estas no persistir치n si se borra todo. Por otro lado, cualquier conexi칩n
guardada en `secrets/connections.yaml` no aparecer치 en la UI, aunque eso no significa que no exista. Consulten la
documentaci칩n para obtener m치s
[informaci칩n](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/connections.html).

## Conexi칩n con los buckets

Dado que no estamos utilizando Amazon S3, sino una implementaci칩n local de los mismos mediante MinIO, es necesario
modificar las variables de entorno para conectar con el servicio de MinIO. Las variables de entorno son las siguientes:

```bash
AWS_ACCESS_KEY_ID=minio   
AWS_SECRET_ACCESS_KEY=minio123 
AWS_ENDPOINT_URL_S3=http://localhost:90000
```

MLflow tambi칠n tiene una variable de entorno que afecta su conexi칩n a los buckets:

```bash
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
```

Aseg칰rate de establecer estas variables de entorno antes de ejecutar tu notebook o scripts en tu m치quina o en cualquier
otro lugar. Si est치s utilizando un servidor externo a tu computadora de trabajo, reemplaza localhost por su direcci칩n
IP.

Al hacer esto, podr치s utilizar `boto3`, `awswrangler`, etc., en Python con estos buckets, o `awscli` en la consola.

Si tienes acceso a AWS S3, ten mucho cuidado de no reemplazar tus credenciales de AWS. Si usas las variables de entorno,
no tendr치s problemas.

## Valkey

La base de datos Valkey es usada por Apache Airflow para su funcionamiento. Tal como est치 configurado ahora no esta
expuesto el puerto para poder ser usado externamente. Se puede modificar el archivo `docker-compose.yaml` para
habilitaro.

## Pull Request

Este repositorio est치 abierto para que realicen sus propios Pull Requests y as칤 contribuir a mejorarlo. Si desean
realizar alguna modificaci칩n, **춰son bienvenidos!** Tambi칠n se pueden crear nuevos entornos productivos para aumentar la
variedad de implementaciones, idealmente en diferentes `branches`. Algunas ideas que se me ocurren que podr칤an
implementar son:

- Reemplazar Airflow y MLflow con [Metaflow](https://metaflow.org/) o [Kubeflow](https://www.kubeflow.org).
- Reemplazar MLflow con [Seldon-Core](https://github.com/SeldonIO/seldon-core).
- Agregar un servicio de tableros como, por ejemplo, [Grafana](https://grafana.com).

## Actualizaciones

Para utilizar este repositorio y cargar datos del dataset `ashraq/fashion-product-images-small`, se puede ejecutar el
siguiente comando:

```bash
poetry run python src/data/dataset_loader.py
```

Este script realiza las siguientes tareas:

- Descarga un conjunto de im치genes del dataset desde Hugging Face.
- Guarda las im치genes en un bucket S3 utilizando MinIO como almacenamiento.
- Genera un 칤ndice en PostgreSQL, preservando las columnas originales del dataset y agregando campos adicionales de
  metadatos.

---

### Verificar los datos indexados en PostgreSQL

Una vez ejecutado el script, se puede consultar el 칤ndice generado en PostgreSQL con:

```bash
psql -h localhost -p 15432 -U airflow -d airflow
```

Y luego ejecutar la siguiente consulta SQL:

```sql
SELECT * FROM fashion_files LIMIT 5;
```

---

### Instalaci칩n de `psql` en macOS

Para poder ejecutar `psql`, es necesario tenerlo instalado. En macOS, se puede instalar con:

```bash
brew install libpq
```

Como `libpq` es un paquete *keg-only*, no se agrega autom치ticamente al `PATH`. Para solucionarlo, se debe ejecutar:

```bash
echo 'export PATH="/opt/homebrew/opt/libpq/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc
```

Verific치 que haya quedado correctamente configurado con:

```bash
which psql
psql --version
```

La salida esperada deber칤a ser similar a:

```
/opt/homebrew/opt/libpq/bin/psql
psql (PostgreSQL) 17.5
```

Con esto ya pod칠s volver a ejecutar el comando `psql` y realizar consultas sobre la tabla `fashion_files`.

## API GraphQL

Este proyecto expone una API GraphQL desarrollada con **Strawberry** y **FastAPI**, que permite consultar los metadatos
de los archivos indexados del dataset `ashraq/fashion-product-images-small`.

### 游댋 Consultar la API

Docker levantar치 la API en el puerto 8801:8801. Accediendo al endpoint `/graphql` se podr치n ejecutar consultas usando la
UI.

---

### 游늶 Queries disponibles

#### 游댳 `allFiles`

Devuelve todos los registros indexados (limitado por defecto en el backend).

```graphql
{
  allFiles {
    id
    filename
    gender
    masterCategory
    baseColour
  }
}
```

---

#### 游댳 `filesByFilters(...)`

Consulta flexible con m칰ltiples filtros opcionales y paginaci칩n:

**Par치metros disponibles:**

- `masterCategory` (String)
- `gender` (String)
- `baseColour` (String)
- `season` (String)
- `year` (String)
- `limit` (Int, por defecto: 50)
- `offset` (Int, por defecto: 0)

**Ejemplos:**

```graphql
{
  filesByFilters(gender: "Women", season: "Winter", limit: 10) {
    id
    filename
    productDisplayName
  }
}
```

---

### 游닍 Campos disponibles en cada archivo (`FashionFile`)

- `id`
- `filename`
- `s3Path`
- `masterCategory`
- `subCategory`
- `articleType`
- `baseColour`
- `season`
- `year`
- `usage`
- `gender`
- `productDisplayName`
- `dataset`
- `created_at`

---

> 游눠 Nota: los archivos f칤sicos est치n almacenados en un bucket S3 (MinIO), y los campos representan metadatos extra칤dos
> al momento de la carga del dataset.
>> > > > > > be37c6d9145b6afd10c5928ced6e139cf350f759
